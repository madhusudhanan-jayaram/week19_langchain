{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f250c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to import ConversationBufferMemory...\n",
      "âœ“ Found ChatMessageHistory in langchain_community.chat_message_histories\n",
      "Key loaded: AIzaSy...\n",
      "OpenAI Key loaded: sk-pro...\n",
      "OpenAI response: content='IntelliHome Technologies' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 21, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CrZvDmTrMLIWzA3f540P6lFB8nC8D', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b6295-2f89-7312-8e82-8373c75f76eb-0' usage_metadata={'input_tokens': 21, 'output_tokens': 4, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 1: Imports and LLM Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Core Imports\n",
    "# ============================================================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================================================\n",
    "# Text Processing & Embeddings\n",
    "# ============================================================================\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ============================================================================\n",
    "# Vector Stores & Retrieval\n",
    "# ============================================================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "# ============================================================================\n",
    "# Language Models (LLMs)\n",
    "# ============================================================================\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ============================================================================\n",
    "# Prompts & Agents\n",
    "# ============================================================================\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# ============================================================================\n",
    "# Memory & Conversation (for RAG applications)\n",
    "# ============================================================================\n",
    "# Try multiple import paths for ConversationBufferMemory compatibility\n",
    "print(\"Attempting to import ConversationBufferMemory...\")\n",
    "\n",
    "ConversationBufferMemory = None\n",
    "memory_import_paths = [\n",
    "    (\"langchain.memory\", \"ConversationBufferMemory\"),\n",
    "    (\"langchain_community.chat_message_histories\", \"ChatMessageHistory\"),\n",
    "    (\"langchain_core.messages\", \"BaseMessage\"),\n",
    "]\n",
    "\n",
    "for module_name, class_name in memory_import_paths:\n",
    "    try:\n",
    "        module = __import__(module_name, fromlist=[class_name])\n",
    "        ConversationBufferMemory = getattr(module, class_name, None)\n",
    "        if ConversationBufferMemory:\n",
    "            print(f\"âœ“ Found {class_name} in {module_name}\")\n",
    "            break\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "if ConversationBufferMemory is None:\n",
    "    print(\"âš ï¸  ConversationBufferMemory not found - will implement custom solution\")\n",
    "\n",
    "# InMemorySaver: Checkpoint memory for agent state\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# API Key Configuration\n",
    "# ============================================================================\n",
    "# Retrieve API keys for Gemini and OpenAI from environment variables\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate that both API keys are present\n",
    "assert GEMINI_API_KEY, \"GEMINI_API_KEY is missing. Check your .env file.\"\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY is missing. Check your .env file.\"\n",
    "\n",
    "# Display partial keys to confirm successful loading (for security, only show first 6 chars)\n",
    "print(\"Key loaded:\", GEMINI_API_KEY[:6] + \"...\" )\n",
    "print(\"OpenAI Key loaded:\", OPENAI_API_KEY[:6] + \"...\")\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize Language Models (LLMs)\n",
    "# ============================================================================\n",
    "# Initialize Google's Gemini LLM with temperature=0 for deterministic responses\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,  # Deterministic output (no randomness)\n",
    "    model=\"gemini-2.5-flash\",  # Using the faster Gemini model\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize OpenAI's GPT-3.5-turbo LLM\n",
    "openai_llm = ChatOpenAI(\n",
    "    temperature=0,  # Deterministic output\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Create a Simple Prompt Template\n",
    "# ============================================================================\n",
    "# Define a reusable prompt template for company name generation\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],  # Variable to be filled dynamically\n",
    "    template=\"Give me a creative name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Example 1: LLM Chain (Prompt â†’ LLM)\n",
    "# ============================================================================\n",
    "# Create a chain: prompt template piped to OpenAI LLM\n",
    "chain = prompt | openai_llm\n",
    "\n",
    "# Execute the chain with a specific product type\n",
    "response = chain.invoke({\"product\": \"smart home devices\"})\n",
    "\n",
    "# Display the generated company name\n",
    "print(\"OpenAI response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d11f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document from sample.txt...\n",
      "âœ“ Document loaded successfully\n",
      "\n",
      "Splitting text into chunks...\n",
      "âœ“ Created 1 document chunks\n",
      "\n",
      "Creating embeddings and FAISS vector store...\n",
      "âœ“ FAISS vector store created successfully\n",
      "\n",
      "Creating retriever...\n",
      "âœ“ Retriever configured (returns top 4 documents)\n",
      "\n",
      "Creating RAG tool...\n",
      "âœ“ RAG tool created successfully\n",
      "\n",
      "Initializing agent with RAG tool...\n",
      "âœ“ Agent created successfully\n",
      "\n",
      "================================================================================\n",
      "TESTING RAG AGENT WITH FAISS VECTOR STORE\n",
      "================================================================================\n",
      "\n",
      "Query: What is LangChain?\n",
      "Answer: LangChain is a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "Query: Why do we use LangChain?\n",
      "Answer: LangChain is used as a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports various features such as RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow applications.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 2: FAISS Vector Database & Retriever\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Load Document\n",
    "# ============================================================================\n",
    "# Read the sample.txt file containing text data to be indexed\n",
    "print(\"Loading document from sample.txt...\")\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "print(\"âœ“ Document loaded successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Split Text into Chunks\n",
    "# ============================================================================\n",
    "# Initialize text splitter to break documents into manageable chunks\n",
    "# - separator=\"\\n\": Split on newline characters\n",
    "# - chunk_size=300: Each chunk contains ~300 characters\n",
    "# - chunk_overlap=50: Overlap for context preservation\n",
    "print(\"Splitting text into chunks...\")\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "# Create document objects from text\n",
    "docs = splitter.create_documents([text_data])\n",
    "print(f\"âœ“ Created {len(docs)} document chunks\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create Embeddings and FAISS Vector Store\n",
    "# ============================================================================\n",
    "# Initialize OpenAI embeddings (converts text to vectors)\n",
    "print(\"Creating embeddings and FAISS vector store...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create FAISS vector database from documents\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "print(\"âœ“ FAISS vector store created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create Retriever\n",
    "# ============================================================================\n",
    "# Create a retriever that fetches relevant documents\n",
    "# search_kwargs={\"k\": 4} means return top 4 most relevant documents\n",
    "print(\"Creating retriever...\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"âœ“ Retriever configured (returns top 4 documents)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Convert Retriever to RAG Tool\n",
    "# ============================================================================\n",
    "# Wrap the retriever as a tool for the agent to use\n",
    "print(\"Creating RAG tool...\")\n",
    "rag_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"rag_qa_tool\",\n",
    "    description=\"Use this tool to answer questions about LangChain using the sample.txt document\"\n",
    ")\n",
    "print(\"âœ“ RAG tool created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 6: Create Agent with RAG Tool\n",
    "# ============================================================================\n",
    "# Create modern agent that can use the RAG tool\n",
    "print(\"Initializing agent with RAG tool...\")\n",
    "agent = create_agent(\n",
    "    openai_llm,  # Use OpenAI LLM\n",
    "    tools=[rag_tool],  # Provide RAG tool\n",
    "    system_prompt=\"You are a helpful assistant. Use the RAG tool when needed to answer questions accurately based on the provided documents.\"\n",
    ")\n",
    "print(\"âœ“ Agent created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 7: Test the RAG Agent\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING RAG AGENT WITH FAISS VECTOR STORE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test Query 1\n",
    "query1 = \"What is LangChain?\"\n",
    "print(f\"Query: {query1}\")\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query1}]\n",
    "})\n",
    "print(f\"Answer: {result1['messages'][-1].content}\\n\")\n",
    "\n",
    "# Test Query 2\n",
    "query2 = \"Why do we use LangChain?\"\n",
    "print(f\"Query: {query2}\")\n",
    "result2 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query2}]\n",
    "})\n",
    "print(f\"Answer: {result2['messages'][-1].content}\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "pw3m46i1gte",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IMPLEMENTING CONVERSATION MEMORY WITH RAG AGENT\n",
      "================================================================================\n",
      "\n",
      "Setting up conversation memory...\n",
      "\n",
      "âœ“ Using LangChain's ConversationBufferMemory\n",
      "\n",
      "Creating conversational agent with memory...\n",
      "\n",
      "âœ“ Conversational agent created\n",
      "  â†’ Agent: create_agent() with RAG tool\n",
      "  â†’ Memory: Conversation Buffer Memory\n",
      "  â†’ Mode: Multi-turn conversation capable\n",
      "\n",
      "Creating conversation handler function...\n",
      "\n",
      "âœ“ Chat function created\n",
      "\n",
      "================================================================================\n",
      "MULTI-TURN CONVERSATION WITH MEMORY\n",
      "================================================================================\n",
      "\n",
      "TURN 1:\n",
      "--------------------------------------------------------------------------------\n",
      "User: What is LangChain?\n",
      "  [Could not format memory: Got unsupported message type: {'role': 'user', 'content': 'What is LangChain?'}]\n",
      "Agent: LangChain is a framework created by Harrison Chase for building applications with Large Language Models (LLMs). It supports RAG, agents, memory, tools, and more, and is commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "TURN 2:\n",
      "--------------------------------------------------------------------------------\n",
      "User: What are the main features of it?\n",
      "  [Could not format memory: Got unsupported message type: {'role': 'user', 'content': 'What is LangChain?'}]\n",
      "Agent: The main features of LangChain include support for RAG, agents, memory, tools, and more. It is commonly used in chatbots, document Q&A, and AI workflow. LangChain is a framework for building applications with Large Language Models (LLMs) and was created by Harrison Chase.\n",
      "\n",
      "TURN 3:\n",
      "--------------------------------------------------------------------------------\n",
      "User: Can you tell me who created it?\n",
      "  [Could not format memory: Got unsupported message type: {'role': 'user', 'content': 'What is LangChain?'}]\n",
      "Agent: LangChain was created by Harrison Chase. It is a framework for building applications with LLMs and supports various features like RAG, agents, memory, tools, and more. It is commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "================================================================================\n",
      "COMPLETE CONVERSATION HISTORY\n",
      "================================================================================\n",
      "\n",
      "  [Could not format memory: Got unsupported message type: {'role': 'user', 'content': 'What is LangChain?'}]\n",
      "[No conversation yet]\n",
      "\n",
      "================================================================================\n",
      "CONVERSATION STATISTICS\n",
      "================================================================================\n",
      "\n",
      "Total Messages: 6\n",
      "  â€¢ User Messages: 0\n",
      "  â€¢ AI Messages: 0\n",
      "  â€¢ Conversation Turns: 0\n",
      "\n",
      "Memory Usage:\n",
      "  â€¢ Characters: 0\n",
      "  â€¢ Words: 0\n",
      "  â€¢ Estimated Tokens: ~0\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 3: ConversationBufferMemory Integration\n",
    "# ============================================================================\n",
    "# This cell adds memory to the agent for multi-turn conversations\n",
    "\n",
    "from typing import List, Dict, Any\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTING CONVERSATION MEMORY WITH RAG AGENT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Create Custom ConversationBufferMemory Class\n",
    "# ============================================================================\n",
    "# If ConversationBufferMemory is not available, use custom implementation\n",
    "\n",
    "print(\"Setting up conversation memory...\\n\")\n",
    "\n",
    "# Helper function to get memory as string\n",
    "def get_memory_string(memory_obj) -> str:\n",
    "    \"\"\"Extract memory history as string from various memory formats.\"\"\"\n",
    "    try:\n",
    "        # Try buffer attribute (LangChain ConversationBufferMemory)\n",
    "        if hasattr(memory_obj, 'buffer'):\n",
    "            return memory_obj.buffer\n",
    "        # Try buffer_as_str property\n",
    "        elif hasattr(memory_obj, 'buffer_as_str'):\n",
    "            return memory_obj.buffer_as_str\n",
    "        # Try chat_memory.messages\n",
    "        elif hasattr(memory_obj, 'chat_memory'):\n",
    "            messages = memory_obj.chat_memory.messages\n",
    "            lines = []\n",
    "            for msg in messages:\n",
    "                role = \"User\" if msg.type == \"human\" else \"Assistant\"\n",
    "                lines.append(f\"{role}: {msg.content}\")\n",
    "            return \"\\n\".join(lines)\n",
    "        # Fallback\n",
    "        else:\n",
    "            return str(memory_obj)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Could not format memory: {e}]\")\n",
    "        return \"\"\n",
    "\n",
    "# Helper function to get messages list\n",
    "def get_memory_messages(memory_obj) -> list:\n",
    "    \"\"\"Extract messages from various memory formats.\"\"\"\n",
    "    try:\n",
    "        if hasattr(memory_obj, 'chat_memory') and hasattr(memory_obj.chat_memory, 'messages'):\n",
    "            return memory_obj.chat_memory.messages\n",
    "        elif hasattr(memory_obj, 'messages'):\n",
    "            return memory_obj.messages\n",
    "        else:\n",
    "            return []\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "if ConversationBufferMemory is not None:\n",
    "    # Use built-in ConversationBufferMemory if available\n",
    "    print(\"âœ“ Using LangChain's ConversationBufferMemory\\n\")\n",
    "    \n",
    "    memory = ConversationBufferMemory(\n",
    "        return_messages=True\n",
    "    )\n",
    "else:\n",
    "    # Implement custom conversation memory\n",
    "    print(\"âš ï¸  Using custom ConversationMemory implementation\\n\")\n",
    "    \n",
    "    class ConversationMemory:\n",
    "        \"\"\"\n",
    "        Custom conversation memory implementation.\n",
    "        Stores complete conversation history with timestamps.\n",
    "        \"\"\"\n",
    "        def __init__(self, memory_key: str = \"chat_history\"):\n",
    "            self.memory_key = memory_key\n",
    "            self.messages: List[Dict[str, Any]] = []\n",
    "        \n",
    "        def add_user_message(self, content: str) -> None:\n",
    "            \"\"\"Add a user message to memory.\"\"\"\n",
    "            self.messages.append({\n",
    "                \"type\": \"human\",\n",
    "                \"content\": content,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        def add_ai_message(self, content: str) -> None:\n",
    "            \"\"\"Add an AI message to memory.\"\"\"\n",
    "            self.messages.append({\n",
    "                \"type\": \"ai\",\n",
    "                \"content\": content,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        @property\n",
    "        def buffer(self) -> str:\n",
    "            \"\"\"Return conversation history as formatted string.\"\"\"\n",
    "            if not self.messages:\n",
    "                return \"\"\n",
    "            \n",
    "            lines = []\n",
    "            for msg in self.messages:\n",
    "                role = \"User\" if msg[\"type\"] == \"human\" else \"Assistant\"\n",
    "                lines.append(f\"{role}: {msg['content']}\")\n",
    "            return \"\\n\".join(lines)\n",
    "    \n",
    "    # Create memory instance\n",
    "    memory = ConversationMemory()\n",
    "    print(\"âœ“ Custom ConversationMemory created\")\n",
    "    print(\"  â†’ Stores complete conversation history\")\n",
    "    print(\"  â†’ Includes timestamps for each message\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Create Conversational RAG Agent with Memory\n",
    "# ============================================================================\n",
    "print(\"Creating conversational agent with memory...\\n\")\n",
    "\n",
    "# System prompt that tells agent about memory and RAG tool\n",
    "system_prompt = \"\"\"You are a helpful assistant that remembers the conversation history.\n",
    "\n",
    "IMPORTANT RULES:\n",
    "1. Always refer to previous messages in the conversation when relevant\n",
    "2. Use the RAG tool to answer questions about LangChain\n",
    "3. Acknowledge when you're using information from previous turns\n",
    "4. Be conversational and maintain context across turns\n",
    "5. If asked about previous questions, reference them naturally\n",
    "\"\"\"\n",
    "\n",
    "# Create agent with memory-aware system prompt\n",
    "conversational_agent = create_agent(\n",
    "    openai_llm,\n",
    "    tools=[rag_tool],\n",
    "    system_prompt=system_prompt\n",
    ")\n",
    "\n",
    "print(\"âœ“ Conversational agent created\")\n",
    "print(\"  â†’ Agent: create_agent() with RAG tool\")\n",
    "print(\"  â†’ Memory: Conversation Buffer Memory\")\n",
    "print(\"  â†’ Mode: Multi-turn conversation capable\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Define Helper Function for Conversational Interactions\n",
    "# ============================================================================\n",
    "print(\"Creating conversation handler function...\\n\")\n",
    "\n",
    "def chat_with_memory(query: str, show_history: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Send a query to the conversational agent and update memory.\n",
    "    \n",
    "    Args:\n",
    "        query (str): User's question/message\n",
    "        show_history (bool): Whether to display conversation history\n",
    "    \n",
    "    Returns:\n",
    "        str: Agent's response\n",
    "    \"\"\"\n",
    "    # Add user message to memory\n",
    "    if hasattr(memory, 'add_message'):\n",
    "        memory.add_message({\"role\": \"user\", \"content\": query})\n",
    "    elif hasattr(memory, 'add_user_message'):\n",
    "        memory.add_user_message(query)\n",
    "    elif hasattr(memory, 'chat_memory'):\n",
    "        memory.chat_memory.add_user_message(query)\n",
    "    else:\n",
    "        print(\"Warning: Could not add user message to memory\")\n",
    "    \n",
    "    # Get conversation history for context\n",
    "    chat_history_str = get_memory_string(memory)\n",
    "    \n",
    "    # Display history if requested\n",
    "    if show_history and chat_history_str:\n",
    "        print(\"\\nðŸ“œ CONVERSATION HISTORY:\")\n",
    "        print(\"â”€\" * 80)\n",
    "        print(chat_history_str)\n",
    "        print(\"â”€\" * 80 + \"\\n\")\n",
    "    \n",
    "    # Invoke agent with query and memory context\n",
    "    try:\n",
    "        result = conversational_agent.invoke({\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "        })\n",
    "        \n",
    "        # Extract agent response\n",
    "        agent_response = result['messages'][-1].content\n",
    "        \n",
    "        # Add agent response to memory\n",
    "        if hasattr(memory, 'add_message'):\n",
    "            memory.add_message({\"role\": \"assistant\", \"content\": agent_response})\n",
    "        elif hasattr(memory, 'add_ai_message'):\n",
    "            memory.add_ai_message(agent_response)\n",
    "        elif hasattr(memory, 'chat_memory'):\n",
    "            memory.chat_memory.add_ai_message(agent_response)\n",
    "        \n",
    "        return agent_response\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"âœ“ Chat function created\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Multi-Turn Conversation Examples\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"MULTI-TURN CONVERSATION WITH MEMORY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Turn 1\n",
    "print(\"TURN 1:\")\n",
    "print(\"-\" * 80)\n",
    "query1 = \"What is LangChain?\"\n",
    "print(f\"User: {query1}\")\n",
    "response1 = chat_with_memory(query1, show_history=False)\n",
    "print(f\"Agent: {response1}\\n\")\n",
    "\n",
    "# Turn 2: References previous conversation\n",
    "print(\"TURN 2:\")\n",
    "print(\"-\" * 80)\n",
    "query2 = \"What are the main features of it?\"\n",
    "print(f\"User: {query2}\")\n",
    "response2 = chat_with_memory(query2, show_history=True)\n",
    "print(f\"Agent: {response2}\\n\")\n",
    "\n",
    "# Turn 3: Another follow-up\n",
    "print(\"TURN 3:\")\n",
    "print(\"-\" * 80)\n",
    "query3 = \"Can you tell me who created it?\"\n",
    "print(f\"User: {query3}\")\n",
    "response3 = chat_with_memory(query3, show_history=False)\n",
    "print(f\"Agent: {response3}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Display Complete Conversation History\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"COMPLETE CONVERSATION HISTORY\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "full_history = get_memory_string(memory)\n",
    "print(full_history if full_history else \"[No conversation yet]\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 6: Memory Statistics\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CONVERSATION STATISTICS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Get messages\n",
    "messages = get_memory_messages(memory)\n",
    "\n",
    "# Count message types\n",
    "num_user_messages = 0\n",
    "num_ai_messages = 0\n",
    "\n",
    "for m in messages:\n",
    "    # Handle both dict and object formats\n",
    "    msg_type = None\n",
    "    if isinstance(m, dict):\n",
    "        msg_type = m.get(\"type\")\n",
    "    elif hasattr(m, 'type'):\n",
    "        msg_type = m.type\n",
    "    \n",
    "    if msg_type == \"human\":\n",
    "        num_user_messages += 1\n",
    "    elif msg_type == \"ai\":\n",
    "        num_ai_messages += 1\n",
    "\n",
    "print(f\"Total Messages: {len(messages)}\")\n",
    "print(f\"  â€¢ User Messages: {num_user_messages}\")\n",
    "print(f\"  â€¢ AI Messages: {num_ai_messages}\")\n",
    "print(f\"  â€¢ Conversation Turns: {num_user_messages}\")\n",
    "\n",
    "# Estimate token count\n",
    "total_text = full_history if full_history else \"\"\n",
    "word_count = len(total_text.split()) if total_text else 0\n",
    "estimated_tokens = int(word_count * 1.3)\n",
    "\n",
    "print(f\"\\nMemory Usage:\")\n",
    "print(f\"  â€¢ Characters: {len(total_text):,}\")\n",
    "print(f\"  â€¢ Words: {word_count:,}\")\n",
    "print(f\"  â€¢ Estimated Tokens: ~{estimated_tokens:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
