{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f250c5c",
   "metadata": {},
   "outputs": [],
   "source": "# ============================================================================\n# LangChain RAG Tool Setup - Cell 1: Imports and LLM Configuration\n# ============================================================================\n\n# ============================================================================\n# Core Imports\n# ============================================================================\nimport os\nfrom dotenv import load_dotenv\n\n# ============================================================================\n# Text Processing & Embeddings\n# ============================================================================\nfrom langchain_text_splitters import CharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\n\n# ============================================================================\n# Vector Stores & Retrieval\n# ============================================================================\nfrom langchain_community.vectorstores import FAISS\nfrom langchain_core.tools import create_retriever_tool\n\n# ============================================================================\n# Language Models (LLMs)\n# ============================================================================\nfrom langchain_openai import ChatOpenAI\nfrom langchain_google_genai import ChatGoogleGenerativeAI\n\n# ============================================================================\n# Prompts & Agents\n# ============================================================================\nfrom langchain_core.prompts import PromptTemplate, ChatPromptTemplate\nfrom langchain.agents import create_agent\n\n# ============================================================================\n# Checkpoint & Memory (for agent state)\n# ============================================================================\nfrom langgraph.checkpoint.memory import InMemorySaver\n\n# Load environment variables from .env file\nload_dotenv()\n\n# ============================================================================\n# API Key Configuration\n# ============================================================================\n# Retrieve API keys for Gemini and OpenAI from environment variables\nGEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Validate that both API keys are present\nassert GEMINI_API_KEY, \"GEMINI_API_KEY is missing. Check your .env file.\"\nassert OPENAI_API_KEY, \"OPENAI_API_KEY is missing. Check your .env file.\"\n\n# Display partial keys to confirm successful loading (for security, only show first 6 chars)\nprint(\"Key loaded:\", GEMINI_API_KEY[:6] + \"...\" )\nprint(\"OpenAI Key loaded:\", OPENAI_API_KEY[:6] + \"...\")\n\n# ============================================================================\n# Initialize Language Models (LLMs)\n# ============================================================================\n# Initialize Google's Gemini LLM with temperature=0 for deterministic responses\ngemini_llm = ChatGoogleGenerativeAI(\n    temperature=0,  # Deterministic output (no randomness)\n    model=\"gemini-2.5-flash\",  # Using the faster Gemini model\n    google_api_key=GEMINI_API_KEY\n)\n\n# Initialize OpenAI's GPT-3.5-turbo LLM\nopenai_llm = ChatOpenAI(\n    temperature=0,  # Deterministic output\n    model=\"gpt-3.5-turbo\",\n    openai_api_key=OPENAI_API_KEY\n)\n\n# ============================================================================\n# Create a Simple Prompt Template\n# ============================================================================\n# Define a reusable prompt template for company name generation\nprompt = PromptTemplate(\n    input_variables=[\"product\"],  # Variable to be filled dynamically\n    template=\"Give me a creative name for a company that makes {product}?\",\n)\n\n# ============================================================================\n# Example 1: LLM Chain (Prompt â†’ LLM)\n# ============================================================================\n# Create a chain: prompt template piped to OpenAI LLM\nchain = prompt | openai_llm\n\n# Execute the chain with a specific product type\nresponse = chain.invoke({\"product\": \"smart home devices\"})\n\n# Display the generated company name\nprint(\"OpenAI response:\", response)"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d11f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 2: Retrieval-Augmented Generation (RAG)\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Load and Process Document\n",
    "# ============================================================================\n",
    "# Read the sample.txt file containing text data to be indexed\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Split Text into Chunks\n",
    "# ============================================================================\n",
    "# Initialize text splitter to break large documents into manageable chunks\n",
    "# - separator=\"\\n\": Split on newline characters\n",
    "# - chunk_size=300: Each chunk contains ~300 characters\n",
    "# - chunk_overlap=50: Chunks overlap by 50 characters to maintain context\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\", chunk_size=300, chunk_overlap=50)\n",
    "docs = splitter.create_documents([text_data])\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create Embeddings and Vector Store\n",
    "# ============================================================================\n",
    "# Initialize OpenAI embeddings (converts text to numerical vectors)\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create a FAISS vector database from the document chunks\n",
    "# FAISS enables fast similarity search on the embedded documents\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "\n",
    "# Create a retriever that fetches the top 4 most relevant documents\n",
    "# when a query is performed (search_kwargs={\"k\": 4})\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create RAG Tool\n",
    "# ============================================================================\n",
    "# Convert the retriever into a tool that an agent can use\n",
    "# This tool will retrieve relevant documents from sample.txt\n",
    "rag_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"langchain_rag\",  # Tool name for the agent\n",
    "    description=\"Use this to answer questions using the content in sample.txt\"  # Tool description\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Create an Agent with Tools\n",
    "# ============================================================================\n",
    "# Create a modern LangChain agent that can use tools (like our RAG tool)\n",
    "# This replaces the older create_tool_calling_agent + AgentExecutor pattern\n",
    "agent = create_agent(\n",
    "    openai_llm,  # Use OpenAI's LLM to power the agent\n",
    "    tools=[rag_tool],  # Provide the RAG tool for document retrieval\n",
    "    system_prompt=\"You are a helpful assistant. Use tools when needed.\"  # Agent behavior instruction\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 6: Invoke the Agent\n",
    "# ============================================================================\n",
    "# Execute the agent with a user query using the messages format\n",
    "# The agent will automatically decide when to use the RAG tool\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"What is LangChain and why do we use it?\"}]\n",
    "})\n",
    "\n",
    "# Extract and display the final answer from the agent's response\n",
    "# The last message in the response contains the agent's final answer\n",
    "print(result[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "id": "t74mozhu3vk",
   "source": "# ============================================================================\n# WHY WE NEED CONTEXTUAL COMPRESSION RETRIEVER\n# ============================================================================\n# Understanding the problem it solves and when to use it\n\nfrom tabulate import tabulate\n\nprint(\"\\n\" + \"â–ˆ\"*150)\nprint(\"â–ˆ\" + \" \"*148 + \"â–ˆ\")\nprint(\"â–ˆ\" + \"CONTEXTUAL COMPRESSION RETRIEVER: SOLVING REAL-WORLD PROBLEMS\".center(148) + \"â–ˆ\")\nprint(\"â–ˆ\" + \" \"*148 + \"â–ˆ\")\nprint(\"â–ˆ\"*150 + \"\\n\")\n\n# ============================================================================\n# DIAGRAM 1: THE PROBLEM\n# ============================================================================\nprint(\"â”Œ\" + \"â”€\"*148 + \"â”\")\nprint(\"â”‚\" + \"DIAGRAM 1: THE PROBLEM - WASTED TOKENS & NOISE\".ljust(148) + \"â”‚\")\nprint(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n\nproblem_diagram = \"\"\"\nUSER QUERY: \"What is LangChain?\"\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  REGULAR RETRIEVER (WITHOUT COMPRESSION)                                                    â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nRetrieved Document 1 (500 tokens):\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ LangChain was founded in 2022... [100 words about funding]... Series A funding announced â”‚\nâ”‚ in 2023... [50 words about investors]... Headquarters in San Francisco... [lots more      â”‚\nâ”‚ irrelevant details]... âœ“ KEY INFO: \"LangChain is a framework for LLMs\"                   â”‚\nâ”‚ [200 more words about company history]...                                                 â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ”´ Problem:\n  â€¢ 500 tokens sent to LLM\n  â€¢ Only 1-2 sentences are relevant\n  â€¢ 95% is NOISE/WASTED tokens\n  â€¢ Costs: $0.15 per 1000 tokens (GPT-4) â†’ Wastes money!\n  â€¢ LLM context gets cluttered\n  â€¢ Potential for hallucination from irrelevant content\n\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚  CONTEXTUAL COMPRESSION RETRIEVER (WITH COMPRESSION)                                        â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nSame Retrieved Document (500 tokens) â†’ LLM Compression:\n\n\"Extract ONLY the parts relevant to: 'What is LangChain?'\"\n\nResult (50 tokens):\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ LangChain is a framework for building applications with Large Language Models. It was   â”‚\nâ”‚ founded in 2022 and supports RAG, agents, memory, tools, and more.                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâœ… Solution:\n  â€¢ Only 50 tokens sent to LLM\n  â€¢ 100% relevant content\n  â€¢ 90% token reduction!\n  â€¢ Saves money: 9x cheaper for this retrieval\n  â€¢ Cleaner context = Better answers\n  â€¢ Less hallucination risk\n\"\"\"\n\nprint(problem_diagram)\n\n# ============================================================================\n# DIAGRAM 2: TOKEN COST COMPARISON\n# ============================================================================\nprint(\"\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\nprint(\"â”‚\" + \"DIAGRAM 2: REAL-WORLD TOKEN & COST COMPARISON\".ljust(148) + \"â”‚\")\nprint(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n\ncost_diagram = \"\"\"\nScenario: 10 queries to a knowledge base with 100-doc context\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ REGULAR RETRIEVER (No Compression)                                                           â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Per Query:                                                                                   â”‚\nâ”‚   â€¢ Retrieve 4 documents Ã— 500 tokens average = 2,000 tokens                               â”‚\nâ”‚   â€¢ Send to LLM = 2,000 tokens (input)                                                      â”‚\nâ”‚   â€¢ LLM response = 200 tokens (output)                                                      â”‚\nâ”‚   â€¢ Total per query = 2,200 tokens                                                          â”‚\nâ”‚                                                                                              â”‚\nâ”‚ For 10 queries:                                                                              â”‚\nâ”‚   â€¢ Total tokens = 22,000 tokens                                                            â”‚\nâ”‚   â€¢ Cost (GPT-4): 22,000 Ã— $0.03/1K = $0.66                                                â”‚\nâ”‚   â€¢ Cost (GPT-4 Turbo): 22,000 Ã— $0.01/1K = $0.22                                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚ CONTEXTUAL COMPRESSION RETRIEVER (With Compression)                                          â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚ Per Query:                                                                                   â”‚\nâ”‚   â€¢ Retrieve 4 documents = 2,000 tokens (not sent to LLM, used for compression)             â”‚\nâ”‚   â€¢ LLM compresses â†’ 250 tokens average (90% reduction)                                     â”‚\nâ”‚   â€¢ Send compressed docs to LLM = 250 tokens (input)                                        â”‚\nâ”‚   â€¢ LLM response = 200 tokens (output)                                                      â”‚\nâ”‚   â€¢ Compression cost = 2,000 + 250 = 2,250 tokens                                           â”‚\nâ”‚   â€¢ Response cost = 450 tokens                                                              â”‚\nâ”‚   â€¢ Total per query = 2,700 tokens (includes compression overhead)                          â”‚\nâ”‚                                                                                              â”‚\nâ”‚ For 10 queries:                                                                              â”‚\nâ”‚   â€¢ Total tokens = 27,000 tokens                                                            â”‚\nâ”‚   â€¢ BUT: Only 4,500 tokens sent to main LLM                                                â”‚\nâ”‚   â€¢ Main LLM cost (GPT-4): 4,500 Ã— $0.03/1K = $0.135                                       â”‚\nâ”‚   â€¢ Compression LLM cost: 22,500 Ã— $0.0005/1K (cheaper model) = $0.011                     â”‚\nâ”‚   â€¢ Total cost â‰ˆ $0.15 (70% SAVINGS!)                                                      â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nğŸ’° SAVINGS: $0.66 â†’ $0.15 (77% cheaper!)\n\"\"\"\n\nprint(cost_diagram)\n\n# ============================================================================\n# DIAGRAM 3: COMPARISON TABLE\n# ============================================================================\nprint(\"\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\nprint(\"â”‚\" + \"DIAGRAM 3: DETAILED COMPARISON\".ljust(148) + \"â”‚\")\nprint(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n\ncomparison_data = [\n    [\"Aspect\", \"Regular Retriever\", \"Contextual Compression\"],\n    [\"â”€\" * 30, \"â”€\" * 50, \"â”€\" * 60],\n    [\n        \"Tokens per 4 docs\",\n        \"~2,000 tokens\",\n        \"~250 tokens (90% reduction)\"\n    ],\n    [\n        \"LLM Context\",\n        \"Cluttered with irrelevant info\",\n        \"Clean, focused, relevant only\"\n    ],\n    [\n        \"Cost per query\",\n        \"High (full documents)\",\n        \"Low (compressed documents)\"\n    ],\n    [\n        \"Processing Speed\",\n        \"Fast âš¡âš¡âš¡\",\n        \"Slower âš¡ (needs compression)\"\n    ],\n    [\n        \"Hallucination Risk\",\n        \"Higher (noise confuses LLM)\",\n        \"Lower (only relevant info)\"\n    ],\n    [\n        \"Best For\",\n        \"Simple queries, quick responses\",\n        \"Large docs, token-limited, cost-sensitive\"\n    ],\n    [\n        \"Setup Complexity\",\n        \"Simple\",\n        \"Moderate (needs compressor LLM)\"\n    ],\n    [\n        \"Trade-off\",\n        \"Fast but expensive\",\n        \"Slower but cheaper & cleaner\"\n    ]\n]\n\nprint(tabulate(comparison_data, tablefmt=\"grid\", maxcolwidths=[30, 50, 60]))\n\n# ============================================================================\n# DIAGRAM 4: WHEN TO USE EACH\n# ============================================================================\nprint(\"\\n\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\nprint(\"â”‚\" + \"DIAGRAM 4: DECISION GUIDE - WHEN TO USE WHAT\".ljust(148) + \"â”‚\")\nprint(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n\ndecision_guide = \"\"\"\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                           CHOOSE REGULAR RETRIEVER WHEN:                                     â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  âœ“ Small documents (< 200 tokens average)                                                   â”‚\nâ”‚  âœ“ Need maximum speed (no compression overhead)                                             â”‚\nâ”‚  âœ“ Budget is unlimited                                                                      â”‚\nâ”‚  âœ“ Few documents to retrieve                                                                â”‚\nâ”‚  âœ“ Simple Q&A with high relevance documents                                                 â”‚\nâ”‚  âœ“ Prototyping / Learning (quick setup)                                                     â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                    CHOOSE CONTEXTUAL COMPRESSION WHEN:                                       â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  âœ“ Large documents (> 500 tokens)                                                           â”‚\nâ”‚  âœ“ Token limits (context window constraints)                                                â”‚\nâ”‚  âœ“ Cost-sensitive (production, many queries)                                                â”‚\nâ”‚  âœ“ Documents have mostly irrelevant information                                             â”‚\nâ”‚  âœ“ Need high-quality, focused context                                                       â”‚\nâ”‚  âœ“ Want to reduce hallucination                                                             â”‚\nâ”‚  âœ“ Running on budget (startups, limited resources)                                          â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\nâ”‚                            HYBRID APPROACH:                                                   â”‚\nâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\nâ”‚  Use both: Retrieve 10 docs quickly â†’ Compress top 4 â†’ Send to LLM                         â”‚\nâ”‚  Best of both worlds: Good recall + low cost                                                â”‚\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n\"\"\"\n\nprint(decision_guide)\n\n# ============================================================================\n# CODE EXAMPLE: HOW TO USE IT\n# ============================================================================\nprint(\"\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\nprint(\"â”‚\" + \"DIAGRAM 5: CODE EXAMPLE - IMPLEMENTING CONTEXTUAL COMPRESSION\".ljust(148) + \"â”‚\")\nprint(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n\ncode_example = \"\"\"\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain.retrievers.document_compressors import LLMCompressor\nfrom langchain_openai import ChatOpenAI\n\n# Step 1: Create base retriever\nretriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n\n# Step 2: Create compressor (LLM that compresses documents)\ncompressor_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\ncompressor = LLMCompressor.from_llm(compressor_llm)\n\n# Step 3: Wrap retriever with compression\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)\n\n# Step 4: Use it (same way as regular retriever)\ndocs = compression_retriever.invoke(\"What is LangChain?\")\n\nprint(f\"Retrieved {len(docs)} documents\")\nfor doc in docs:\n    print(f\"Compressed content: {doc.page_content}\")\n    print(f\"Original size was larger, now optimized!\")\n\n\nâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nWHAT HAPPENS INTERNALLY:\n\n1. Your Query\n   â†“\n2. Base Retriever fetches 10 raw documents (5,000 tokens)\n   â†“\n3. LLMCompressor analyzes: \"Which parts are relevant to the query?\"\n   â†“\n4. Compressor creates summaries/extracts relevant parts\n   â†“\n5. Compressed documents returned (500 tokens)\n   â†“\n6. Documents sent to your LLM with 10x fewer tokens!\n\"\"\"\n\nprint(code_example)\n\nprint(\"\\n\" + \"â–ˆ\"*150 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4wjmd6vfqzr",
   "source": "# ============================================================================\n# Retriever Types & When to Use Them - Educational Guide\n# ============================================================================\n# This cell demonstrates different retriever strategies available in LangChain\n\n# Print header\nprint(\"\\n\" + \"=\"*80)\nprint(\"LANGCHAIN RETRIEVER TYPES & USE CASES\")\nprint(\"=\"*80 + \"\\n\")\n\n# ============================================================================\n# 1. SIMILARITY SEARCH (Default)\n# ============================================================================\nprint(\"1. SIMILARITY SEARCH (Default Retriever)\")\nprint(\"-\" * 80)\nprint(\"What it does:\")\nprint(\"  â€¢ Returns the k most similar documents based on vector similarity\")\nprint(\"  â€¢ Ranks results by relevance score (highest to lowest)\")\nprint(\"\\nWhen to use:\")\nprint(\"  âœ“ General use cases and standard applications\")\nprint(\"  âœ“ When you want straightforward relevance ranking\")\nprint(\"  âœ“ Most RAG applications start with this\")\nprint(\"\\nPros:\")\nprint(\"  â€¢ Fast and efficient\")\nprint(\"  â€¢ Simple to understand and use\")\nprint(\"  â€¢ Works well for most scenarios\")\nprint(\"\\nCons:\")\nprint(\"  â€¢ May return redundant/similar results\")\nprint(\"  â€¢ No built-in quality filtering\")\nprint(\"\\nExample:\")\nsimilarity_retriever = vectorstore.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 4}\n)\nprint(\"  retriever = vectorstore.as_retriever(\")\nprint(\"      search_type='similarity',\")\nprint(\"      search_kwargs={'k': 4}\")\nprint(\"  )\")\nprint()\n\n# ============================================================================\n# 2. MAXIMUM MARGINAL RELEVANCE (MMR)\n# ============================================================================\nprint(\"2. MAXIMUM MARGINAL RELEVANCE (MMR) RETRIEVER\")\nprint(\"-\" * 80)\nprint(\"What it does:\")\nprint(\"  â€¢ Returns diverse results by reducing redundancy\")\nprint(\"  â€¢ Balances relevance with diversity\")\nprint(\"  â€¢ Avoids returning very similar documents\")\nprint(\"\\nWhen to use:\")\nprint(\"  âœ“ Want diverse perspectives on a topic\")\nprint(\"  âœ“ Building recommendation systems\")\nprint(\"  âœ“ Need comprehensive coverage, not just top matches\")\nprint(\"\\nPros:\")\nprint(\"  â€¢ More diverse search results\")\nprint(\"  â€¢ Better for exploring multiple angles\")\nprint(\"  â€¢ Reduces information redundancy\")\nprint(\"\\nCons:\")\nprint(\"  â€¢ Slightly slower than similarity search\")\nprint(\"  â€¢ Requires tuning fetch_k parameter\")\nprint(\"\\nExample:\")\nmmr_retriever = vectorstore.as_retriever(\n    search_type=\"mmr\",\n    search_kwargs={\n        \"k\": 4,        # Final number of documents to return\n        \"fetch_k\": 20  # Consider top 20 for diversity\n    }\n)\nprint(\"  retriever = vectorstore.as_retriever(\")\nprint(\"      search_type='mmr',\")\nprint(\"      search_kwargs={\")\nprint(\"          'k': 4,        # Return 4 documents\")\nprint(\"          'fetch_k': 20  # Consider top 20 candidates\")\nprint(\"      }\")\nprint(\"  )\")\nprint()\n\n# ============================================================================\n# 3. SIMILARITY WITH SCORE THRESHOLD\n# ============================================================================\nprint(\"3. SIMILARITY WITH SCORE THRESHOLD\")\nprint(\"-\" * 80)\nprint(\"What it does:\")\nprint(\"  â€¢ Returns only documents with similarity above a threshold\")\nprint(\"  â€¢ Filters out low-quality matches automatically\")\nprint(\"  â€¢ Returns 0 results if nothing meets the threshold\")\nprint(\"\\nWhen to use:\")\nprint(\"  âœ“ Need quality guarantees\")\nprint(\"  âœ“ Want to avoid low-relevance results\")\nprint(\"  âœ“ Building strict compliance/safety systems\")\nprint(\"\\nPros:\")\nprint(\"  â€¢ Guaranteed minimum quality\")\nprint(\"  â€¢ Prevents hallucination from poor matches\")\nprint(\"  â€¢ Clear quality control\")\nprint(\"\\nCons:\")\nprint(\"  â€¢ May return fewer results than expected\")\nprint(\"  â€¢ Requires tuning score_threshold\")\nprint(\"\\nExample:\")\nthreshold_retriever = vectorstore.as_retriever(\n    search_type=\"similarity_score_threshold\",\n    search_kwargs={\n        \"score_threshold\": 0.5,  # Only return if similarity > 0.5\n        \"k\": 4\n    }\n)\nprint(\"  retriever = vectorstore.as_retriever(\")\nprint(\"      search_type='similarity_score_threshold',\")\nprint(\"      search_kwargs={\")\nprint(\"          'score_threshold': 0.5,  # Minimum similarity\")\nprint(\"          'k': 4\")\nprint(\"      }\")\nprint(\"  )\")\nprint()\n\n# ============================================================================\n# VECTOR STORE OPTIONS\n# ============================================================================\nprint(\"VECTOR STORE OPTIONS IN LANGCHAIN\")\nprint(\"-\" * 80)\nprint(\"Different vector databases you can use instead of FAISS:\\n\")\n\nvector_stores = {\n    \"FAISS\": {\n        \"Speed\": \"âš¡âš¡âš¡âš¡âš¡ Fastest\",\n        \"Storage\": \"In-memory only\",\n        \"Persistence\": \"No\",\n        \"Use Case\": \"Development, prototyping\",\n        \"Best For\": \"Quick experiments, local testing\"\n    },\n    \"Chroma\": {\n        \"Speed\": \"âš¡âš¡âš¡âš¡ Fast\",\n        \"Storage\": \"Persistent\",\n        \"Persistence\": \"Yes\",\n        \"Use Case\": \"Development + persistence\",\n        \"Best For\": \"Notebooks, small projects with storage\"\n    },\n    \"Pinecone\": {\n        \"Speed\": \"âš¡âš¡âš¡âš¡ Fast\",\n        \"Storage\": \"Cloud-based\",\n        \"Persistence\": \"Yes\",\n        \"Use Case\": \"Production, large scale\",\n        \"Best For\": \"Production systems, enterprise apps\"\n    },\n    \"Weaviate\": {\n        \"Speed\": \"âš¡âš¡âš¡âš¡ Fast\",\n        \"Storage\": \"Cloud + Self-hosted\",\n        \"Persistence\": \"Yes\",\n        \"Use Case\": \"Production, hybrid search\",\n        \"Best For\": \"Semantic + keyword search\"\n    },\n    \"Qdrant\": {\n        \"Speed\": \"âš¡âš¡âš¡âš¡âš¡ Fastest\",\n        \"Storage\": \"Cloud + Self-hosted\",\n        \"Persistence\": \"Yes\",\n        \"Use Case\": \"High-performance production\",\n        \"Best For\": \"Large-scale, performance-critical apps\"\n    },\n}\n\nfor store_name, features in vector_stores.items():\n    print(f\"\\n{store_name}:\")\n    for key, value in features.items():\n        print(f\"  {key}: {value}\")\n\nprint(\"\\n\")\n\n# ============================================================================\n# QUICK DECISION GUIDE\n# ============================================================================\nprint(\"=\"*80)\nprint(\"QUICK DECISION GUIDE - WHICH RETRIEVER TO USE?\")\nprint(\"=\"*80)\nprint(\"\"\"\nYour Situation                  | Recommended Retriever\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nBuilding a prototype/demo       | Similarity Search + FAISS\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nNeed diverse search results     | MMR Retriever\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nMust filter low-quality results | Similarity with Score Threshold\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nProduction system with storage  | Similarity Search + Chroma/Pinecone\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nHigh-performance production     | Similarity Search + Qdrant\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nHybrid keyword + semantic       | Ensemble Retriever (BM25 + Vector)\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nComplex questions, need recall  | Multi-Query Retriever\nâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"Current Setup in this Notebook:\")\nprint(\"=\"*80)\nprint(\"Vector Store: FAISS (in-memory, fast)\")\nprint(\"Retriever Type: Similarity Search (default)\")\nprint(\"Number of Results: k=4 (returns top 4 most similar documents)\")\nprint(\"\\nâœ“ Good for: Learning, prototyping, quick experiments\")\nprint(\"âœ— Not suitable for: Production systems needing persistence\")\nprint(\"=\"*80 + \"\\n\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}