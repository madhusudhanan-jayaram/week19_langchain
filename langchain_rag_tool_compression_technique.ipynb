{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f250c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: AIzaSy...\n",
      "OpenAI Key loaded: sk-pro...\n",
      "OpenAI response: content='IntelliHome Technologies' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 21, 'total_tokens': 25, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CrZY1xj5hvkj7vqoll7Pqnfg56857', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b627f-40ea-78e3-b2a0-9df42a2f04b1-0' usage_metadata={'input_tokens': 21, 'output_tokens': 4, 'total_tokens': 25, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 1: Imports and LLM Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Core Imports\n",
    "# ============================================================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================================================\n",
    "# Text Processing & Embeddings\n",
    "# ============================================================================\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ============================================================================\n",
    "# Vector Stores & Retrieval\n",
    "# ============================================================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "# ============================================================================\n",
    "# Retriever Strategies & Compression\n",
    "# ============================================================================\n",
    "# NOTE: ContextualCompressionRetriever requires langchain>=0.1.0\n",
    "# If you get import errors, upgrade: pip install --upgrade langchain\n",
    "# For now, commented out - documentation still available below\n",
    "#\n",
    "# try:\n",
    "#     from langchain.retrievers.contextual_compression import ContextualCompressionRetriever\n",
    "#     from langchain.retrievers.document_compressors import LLMCompressor\n",
    "#     COMPRESSION_AVAILABLE = True\n",
    "# except ImportError:\n",
    "#     print(\"âš ï¸  ContextualCompressionRetriever not available in this version\")\n",
    "#     print(\"Upgrade LangChain: pip install --upgrade langchain\")\n",
    "#     COMPRESSION_AVAILABLE = False\n",
    "\n",
    "# ============================================================================\n",
    "# Language Models (LLMs)\n",
    "# ============================================================================\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ============================================================================\n",
    "# Prompts & Agents\n",
    "# ============================================================================\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# ============================================================================\n",
    "# Checkpoint & Memory (for agent state)\n",
    "# ============================================================================\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# API Key Configuration\n",
    "# ============================================================================\n",
    "# Retrieve API keys for Gemini and OpenAI from environment variables\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate that both API keys are present\n",
    "assert GEMINI_API_KEY, \"GEMINI_API_KEY is missing. Check your .env file.\"\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY is missing. Check your .env file.\"\n",
    "\n",
    "# Display partial keys to confirm successful loading (for security, only show first 6 chars)\n",
    "print(\"Key loaded:\", GEMINI_API_KEY[:6] + \"...\" )\n",
    "print(\"OpenAI Key loaded:\", OPENAI_API_KEY[:6] + \"...\")\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize Language Models (LLMs)\n",
    "# ============================================================================\n",
    "# Initialize Google's Gemini LLM with temperature=0 for deterministic responses\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,  # Deterministic output (no randomness)\n",
    "    model=\"gemini-2.5-flash\",  # Using the faster Gemini model\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize OpenAI's GPT-3.5-turbo LLM\n",
    "openai_llm = ChatOpenAI(\n",
    "    temperature=0,  # Deterministic output\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Create a Simple Prompt Template\n",
    "# ============================================================================\n",
    "# Define a reusable prompt template for company name generation\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],  # Variable to be filled dynamically\n",
    "    template=\"Give me a creative name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Example 1: LLM Chain (Prompt â†’ LLM)\n",
    "# ============================================================================\n",
    "# Create a chain: prompt template piped to OpenAI LLM\n",
    "chain = prompt | openai_llm\n",
    "\n",
    "# Execute the chain with a specific product type\n",
    "response = chain.invoke({\"product\": \"smart home devices\"})\n",
    "\n",
    "# Display the generated company name\n",
    "print(\"OpenAI response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d11f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document from sample.txt...\n",
      "âœ“ Document loaded successfully\n",
      "\n",
      "Splitting text into chunks...\n",
      "âœ“ Created 1 document chunks\n",
      "\n",
      "Creating embeddings and FAISS vector store...\n",
      "âœ“ FAISS vector store created successfully\n",
      "\n",
      "Creating retriever...\n",
      "âœ“ Retriever configured (returns top 4 documents)\n",
      "\n",
      "Creating RAG tool...\n",
      "âœ“ RAG tool created successfully\n",
      "\n",
      "Initializing agent with RAG tool...\n",
      "âœ“ Agent created successfully\n",
      "\n",
      "================================================================================\n",
      "TESTING RAG AGENT WITH FAISS VECTOR STORE\n",
      "================================================================================\n",
      "\n",
      "Query: What is LangChain?\n",
      "Answer: LangChain is a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "Query: Why do we use LangChain?\n",
      "Answer: LangChain is used as a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports various features such as RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow applications.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 2: FAISS Vector Database & Retriever\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Load Document\n",
    "# ============================================================================\n",
    "# Read the sample.txt file containing text data to be indexed\n",
    "print(\"Loading document from sample.txt...\")\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "print(\"âœ“ Document loaded successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Split Text into Chunks\n",
    "# ============================================================================\n",
    "# Initialize text splitter to break documents into manageable chunks\n",
    "# - separator=\"\\n\": Split on newline characters\n",
    "# - chunk_size=300: Each chunk contains ~300 characters\n",
    "# - chunk_overlap=50: Overlap for context preservation\n",
    "print(\"Splitting text into chunks...\")\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "# Create document objects from text\n",
    "docs = splitter.create_documents([text_data])\n",
    "print(f\"âœ“ Created {len(docs)} document chunks\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create Embeddings and FAISS Vector Store\n",
    "# ============================================================================\n",
    "# Initialize OpenAI embeddings (converts text to vectors)\n",
    "print(\"Creating embeddings and FAISS vector store...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create FAISS vector database from documents\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "print(\"âœ“ FAISS vector store created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create Retriever\n",
    "# ============================================================================\n",
    "# Create a retriever that fetches relevant documents\n",
    "# search_kwargs={\"k\": 4} means return top 4 most relevant documents\n",
    "print(\"Creating retriever...\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"âœ“ Retriever configured (returns top 4 documents)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Convert Retriever to RAG Tool\n",
    "# ============================================================================\n",
    "# Wrap the retriever as a tool for the agent to use\n",
    "print(\"Creating RAG tool...\")\n",
    "rag_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"rag_qa_tool\",\n",
    "    description=\"Use this tool to answer questions about LangChain using the sample.txt document\"\n",
    ")\n",
    "print(\"âœ“ RAG tool created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 6: Create Agent with RAG Tool\n",
    "# ============================================================================\n",
    "# Create modern agent that can use the RAG tool\n",
    "print(\"Initializing agent with RAG tool...\")\n",
    "agent = create_agent(\n",
    "    openai_llm,  # Use OpenAI LLM\n",
    "    tools=[rag_tool],  # Provide RAG tool\n",
    "    system_prompt=\"You are a helpful assistant. Use the RAG tool when needed to answer questions accurately based on the provided documents.\"\n",
    ")\n",
    "print(\"âœ“ Agent created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 7: Test the RAG Agent\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING RAG AGENT WITH FAISS VECTOR STORE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test Query 1\n",
    "query1 = \"What is LangChain?\"\n",
    "print(f\"Query: {query1}\")\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query1}]\n",
    "})\n",
    "print(f\"Answer: {result1['messages'][-1].content}\\n\")\n",
    "\n",
    "# Test Query 2\n",
    "query2 = \"Why do we use LangChain?\"\n",
    "print(f\"Query: {query2}\")\n",
    "result2 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query2}]\n",
    "})\n",
    "print(f\"Answer: {result2['messages'][-1].content}\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0bd4ej5j3ho",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "â–ˆ                                                                                                                                                    â–ˆ\n",
      "â–ˆ                                           CONTEXTUAL COMPRESSION RETRIEVER: SOLVING REAL-WORLD PROBLEMS                                            â–ˆ\n",
      "â–ˆ                                                                                                                                                    â–ˆ\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚DIAGRAM 1: THE PROBLEM - WASTED TOKENS & NOISE                                                                                                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "\n",
      "USER QUERY: \"What is LangChain?\"\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  REGULAR RETRIEVER (WITHOUT COMPRESSION)                                                    â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Retrieved Document 1 (500 tokens):\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ LangChain was founded in 2022... [100 words about funding]... Series A funding announced â”‚\n",
      "â”‚ in 2023... [50 words about investors]... Headquarters in San Francisco... [lots more      â”‚\n",
      "â”‚ irrelevant details]... âœ“ KEY INFO: \"LangChain is a framework for LLMs\"                   â”‚\n",
      "â”‚ [200 more words about company history]...                                                 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ”´ Problem:\n",
      "  â€¢ 500 tokens sent to LLM\n",
      "  â€¢ Only 1-2 sentences are relevant\n",
      "  â€¢ 95% is NOISE/WASTED tokens\n",
      "  â€¢ Costs: $0.15 per 1000 tokens (GPT-4) â†’ Wastes money!\n",
      "  â€¢ LLM context gets cluttered\n",
      "  â€¢ Potential for hallucination from irrelevant content\n",
      "\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  CONTEXTUAL COMPRESSION RETRIEVER (WITH COMPRESSION)                                        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "Same Retrieved Document (500 tokens) â†’ LLM Compression:\n",
      "\n",
      "\"Extract ONLY the parts relevant to: 'What is LangChain?'\"\n",
      "\n",
      "Result (50 tokens):\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ LangChain is a framework for building applications with Large Language Models. It was   â”‚\n",
      "â”‚ founded in 2022 and supports RAG, agents, memory, tools, and more.                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âœ… Solution:\n",
      "  â€¢ Only 50 tokens sent to LLM\n",
      "  â€¢ 100% relevant content\n",
      "  â€¢ 90% token reduction!\n",
      "  â€¢ Saves money: 9x cheaper for this retrieval\n",
      "  â€¢ Cleaner context = Better answers\n",
      "  â€¢ Less hallucination risk\n",
      "\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚DIAGRAM 2: REAL-WORLD TOKEN & COST COMPARISON                                                                                                       â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "\n",
      "Scenario: 10 queries to a knowledge base with 100-doc context\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ REGULAR RETRIEVER (No Compression)                                                           â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Per Query:                                                                                   â”‚\n",
      "â”‚   â€¢ Retrieve 4 documents Ã— 500 tokens average = 2,000 tokens                               â”‚\n",
      "â”‚   â€¢ Send to LLM = 2,000 tokens (input)                                                      â”‚\n",
      "â”‚   â€¢ LLM response = 200 tokens (output)                                                      â”‚\n",
      "â”‚   â€¢ Total per query = 2,200 tokens                                                          â”‚\n",
      "â”‚                                                                                              â”‚\n",
      "â”‚ For 10 queries:                                                                              â”‚\n",
      "â”‚   â€¢ Total tokens = 22,000 tokens                                                            â”‚\n",
      "â”‚   â€¢ Cost (GPT-4): 22,000 Ã— $0.03/1K = $0.66                                                â”‚\n",
      "â”‚   â€¢ Cost (GPT-4 Turbo): 22,000 Ã— $0.01/1K = $0.22                                          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚ CONTEXTUAL COMPRESSION RETRIEVER (With Compression)                                          â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚ Per Query:                                                                                   â”‚\n",
      "â”‚   â€¢ Retrieve 4 documents = 2,000 tokens (not sent to LLM, used for compression)             â”‚\n",
      "â”‚   â€¢ LLM compresses â†’ 250 tokens average (90% reduction)                                     â”‚\n",
      "â”‚   â€¢ Send compressed docs to LLM = 250 tokens (input)                                        â”‚\n",
      "â”‚   â€¢ LLM response = 200 tokens (output)                                                      â”‚\n",
      "â”‚   â€¢ Compression cost = 2,000 + 250 = 2,250 tokens                                           â”‚\n",
      "â”‚   â€¢ Response cost = 450 tokens                                                              â”‚\n",
      "â”‚   â€¢ Total per query = 2,700 tokens (includes compression overhead)                          â”‚\n",
      "â”‚                                                                                              â”‚\n",
      "â”‚ For 10 queries:                                                                              â”‚\n",
      "â”‚   â€¢ Total tokens = 27,000 tokens                                                            â”‚\n",
      "â”‚   â€¢ BUT: Only 4,500 tokens sent to main LLM                                                â”‚\n",
      "â”‚   â€¢ Main LLM cost (GPT-4): 4,500 Ã— $0.03/1K = $0.135                                       â”‚\n",
      "â”‚   â€¢ Compression LLM cost: 22,500 Ã— $0.0005/1K (cheaper model) = $0.011                     â”‚\n",
      "â”‚   â€¢ Total cost â‰ˆ $0.15 (70% SAVINGS!)                                                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "ğŸ’° SAVINGS: $0.66 â†’ $0.15 (77% cheaper!)\n",
      "\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚DIAGRAM 3: DETAILED COMPARISON                                                                                                                      â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Aspect                         | Regular Retriever                                  | Contextual Compression                                       |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ | â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ | â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Tokens per 4 docs              | ~2,000 tokens                                      | ~250 tokens (90% reduction)                                  |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| LLM Context                    | Cluttered with irrelevant info                     | Clean, focused, relevant only                                |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Cost per query                 | High (full documents)                              | Low (compressed documents)                                   |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Processing Speed               | Fast âš¡âš¡âš¡                                        | Slower âš¡ (needs compression)                                |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Hallucination Risk             | Higher (noise confuses LLM)                        | Lower (only relevant info)                                   |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Best For                       | Simple queries, quick responses                    | Large docs, token-limited, cost-sensitive                    |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Setup Complexity               | Simple                                             | Moderate (needs compressor LLM)                              |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "| Trade-off                      | Fast but expensive                                 | Slower but cheaper & cleaner                                 |\n",
      "+--------------------------------+----------------------------------------------------+--------------------------------------------------------------+\n",
      "\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚DIAGRAM 4: DECISION GUIDE - WHEN TO USE WHAT                                                                                                        â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                           CHOOSE REGULAR RETRIEVER WHEN:                                     â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  âœ“ Small documents (< 200 tokens average)                                                   â”‚\n",
      "â”‚  âœ“ Need maximum speed (no compression overhead)                                             â”‚\n",
      "â”‚  âœ“ Budget is unlimited                                                                      â”‚\n",
      "â”‚  âœ“ Few documents to retrieve                                                                â”‚\n",
      "â”‚  âœ“ Simple Q&A with high relevance documents                                                 â”‚\n",
      "â”‚  âœ“ Prototyping / Learning (quick setup)                                                     â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                    CHOOSE CONTEXTUAL COMPRESSION WHEN:                                       â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  âœ“ Large documents (> 500 tokens)                                                           â”‚\n",
      "â”‚  âœ“ Token limits (context window constraints)                                                â”‚\n",
      "â”‚  âœ“ Cost-sensitive (production, many queries)                                                â”‚\n",
      "â”‚  âœ“ Documents have mostly irrelevant information                                             â”‚\n",
      "â”‚  âœ“ Need high-quality, focused context                                                       â”‚\n",
      "â”‚  âœ“ Want to reduce hallucination                                                             â”‚\n",
      "â”‚  âœ“ Running on budget (startups, limited resources)                                          â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚                            HYBRID APPROACH:                                                   â”‚\n",
      "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
      "â”‚  Use both: Retrieve 10 docs quickly â†’ Compress top 4 â†’ Send to LLM                         â”‚\n",
      "â”‚  Best of both worlds: Good recall + low cost                                                â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "\n",
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚DIAGRAM 5: CODE EXAMPLE - IMPLEMENTING CONTEXTUAL COMPRESSION                                                                                       â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "\n",
      "from langchain.retrievers import ContextualCompressionRetriever\n",
      "from langchain.retrievers.document_compressors import LLMCompressor\n",
      "from langchain_openai import ChatOpenAI\n",
      "\n",
      "# Step 1: Create base retriever\n",
      "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
      "\n",
      "# Step 2: Create compressor (LLM that compresses documents)\n",
      "compressor_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
      "compressor = LLMCompressor.from_llm(compressor_llm)\n",
      "\n",
      "# Step 3: Wrap retriever with compression\n",
      "compression_retriever = ContextualCompressionRetriever(\n",
      "    base_compressor=compressor,\n",
      "    base_retriever=retriever\n",
      ")\n",
      "\n",
      "# Step 4: Use it (same way as regular retriever)\n",
      "docs = compression_retriever.invoke(\"What is LangChain?\")\n",
      "\n",
      "print(f\"Retrieved {len(docs)} documents\")\n",
      "for doc in docs:\n",
      "    print(f\"Compressed content: {doc.page_content}\")\n",
      "    print(f\"Original size was larger, now optimized!\")\n",
      "\n",
      "\n",
      "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
      "\n",
      "WHAT HAPPENS INTERNALLY:\n",
      "\n",
      "1. Your Query\n",
      "   â†“\n",
      "2. Base Retriever fetches 10 raw documents (5,000 tokens)\n",
      "   â†“\n",
      "3. LLMCompressor analyzes: \"Which parts are relevant to the query?\"\n",
      "   â†“\n",
      "4. Compressor creates summaries/extracts relevant parts\n",
      "   â†“\n",
      "5. Compressed documents returned (500 tokens)\n",
      "   â†“\n",
      "6. Documents sent to your LLM with 10x fewer tokens!\n",
      "\n",
      "\n",
      "â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# WHY WE NEED CONTEXTUAL COMPRESSION RETRIEVER\n",
    "# ============================================================================\n",
    "# Understanding the problem it solves and when to use it\n",
    "\n",
    "from tabulate import tabulate\n",
    "\n",
    "print(\"\\n\" + \"â–ˆ\"*150)\n",
    "print(\"â–ˆ\" + \" \"*148 + \"â–ˆ\")\n",
    "print(\"â–ˆ\" + \"CONTEXTUAL COMPRESSION RETRIEVER: SOLVING REAL-WORLD PROBLEMS\".center(148) + \"â–ˆ\")\n",
    "print(\"â–ˆ\" + \" \"*148 + \"â–ˆ\")\n",
    "print(\"â–ˆ\"*150 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGRAM 1: THE PROBLEM\n",
    "# ============================================================================\n",
    "print(\"â”Œ\" + \"â”€\"*148 + \"â”\")\n",
    "print(\"â”‚\" + \"DIAGRAM 1: THE PROBLEM - WASTED TOKENS & NOISE\".ljust(148) + \"â”‚\")\n",
    "print(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n",
    "\n",
    "problem_diagram = \"\"\"\n",
    "USER QUERY: \"What is LangChain?\"\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  REGULAR RETRIEVER (WITHOUT COMPRESSION)                                                    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Retrieved Document 1 (500 tokens):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ LangChain was founded in 2022... [100 words about funding]... Series A funding announced â”‚\n",
    "â”‚ in 2023... [50 words about investors]... Headquarters in San Francisco... [lots more      â”‚\n",
    "â”‚ irrelevant details]... âœ“ KEY INFO: \"LangChain is a framework for LLMs\"                   â”‚\n",
    "â”‚ [200 more words about company history]...                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ”´ Problem:\n",
    "  â€¢ 500 tokens sent to LLM\n",
    "  â€¢ Only 1-2 sentences are relevant\n",
    "  â€¢ 95% is NOISE/WASTED tokens\n",
    "  â€¢ Costs: $0.15 per 1000 tokens (GPT-4) â†’ Wastes money!\n",
    "  â€¢ LLM context gets cluttered\n",
    "  â€¢ Potential for hallucination from irrelevant content\n",
    "\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CONTEXTUAL COMPRESSION RETRIEVER (WITH COMPRESSION)                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Same Retrieved Document (500 tokens) â†’ LLM Compression:\n",
    "\n",
    "\"Extract ONLY the parts relevant to: 'What is LangChain?'\"\n",
    "\n",
    "Result (50 tokens):\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ LangChain is a framework for building applications with Large Language Models. It was   â”‚\n",
    "â”‚ founded in 2022 and supports RAG, agents, memory, tools, and more.                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "âœ… Solution:\n",
    "  â€¢ Only 50 tokens sent to LLM\n",
    "  â€¢ 100% relevant content\n",
    "  â€¢ 90% token reduction!\n",
    "  â€¢ Saves money: 9x cheaper for this retrieval\n",
    "  â€¢ Cleaner context = Better answers\n",
    "  â€¢ Less hallucination risk\n",
    "\"\"\"\n",
    "\n",
    "print(problem_diagram)\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGRAM 2: TOKEN COST COMPARISON\n",
    "# ============================================================================\n",
    "print(\"\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\n",
    "print(\"â”‚\" + \"DIAGRAM 2: REAL-WORLD TOKEN & COST COMPARISON\".ljust(148) + \"â”‚\")\n",
    "print(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n",
    "\n",
    "cost_diagram = \"\"\"\n",
    "Scenario: 10 queries to a knowledge base with 100-doc context\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ REGULAR RETRIEVER (No Compression)                                                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Per Query:                                                                                   â”‚\n",
    "â”‚   â€¢ Retrieve 4 documents Ã— 500 tokens average = 2,000 tokens                               â”‚\n",
    "â”‚   â€¢ Send to LLM = 2,000 tokens (input)                                                      â”‚\n",
    "â”‚   â€¢ LLM response = 200 tokens (output)                                                      â”‚\n",
    "â”‚   â€¢ Total per query = 2,200 tokens                                                          â”‚\n",
    "â”‚                                                                                              â”‚\n",
    "â”‚ For 10 queries:                                                                              â”‚\n",
    "â”‚   â€¢ Total tokens = 22,000 tokens                                                            â”‚\n",
    "â”‚   â€¢ Cost (GPT-4): 22,000 Ã— $0.03/1K = $0.66                                                â”‚\n",
    "â”‚   â€¢ Cost (GPT-4 Turbo): 22,000 Ã— $0.01/1K = $0.22                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ CONTEXTUAL COMPRESSION RETRIEVER (With Compression)                                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Per Query:                                                                                   â”‚\n",
    "â”‚   â€¢ Retrieve 4 documents = 2,000 tokens (not sent to LLM, used for compression)             â”‚\n",
    "â”‚   â€¢ LLM compresses â†’ 250 tokens average (90% reduction)                                     â”‚\n",
    "â”‚   â€¢ Send compressed docs to LLM = 250 tokens (input)                                        â”‚\n",
    "â”‚   â€¢ LLM response = 200 tokens (output)                                                      â”‚\n",
    "â”‚   â€¢ Compression cost = 2,000 + 250 = 2,250 tokens                                           â”‚\n",
    "â”‚   â€¢ Response cost = 450 tokens                                                              â”‚\n",
    "â”‚   â€¢ Total per query = 2,700 tokens (includes compression overhead)                          â”‚\n",
    "â”‚                                                                                              â”‚\n",
    "â”‚ For 10 queries:                                                                              â”‚\n",
    "â”‚   â€¢ Total tokens = 27,000 tokens                                                            â”‚\n",
    "â”‚   â€¢ BUT: Only 4,500 tokens sent to main LLM                                                â”‚\n",
    "â”‚   â€¢ Main LLM cost (GPT-4): 4,500 Ã— $0.03/1K = $0.135                                       â”‚\n",
    "â”‚   â€¢ Compression LLM cost: 22,500 Ã— $0.0005/1K (cheaper model) = $0.011                     â”‚\n",
    "â”‚   â€¢ Total cost â‰ˆ $0.15 (70% SAVINGS!)                                                      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ğŸ’° SAVINGS: $0.66 â†’ $0.15 (77% cheaper!)\n",
    "\"\"\"\n",
    "\n",
    "print(cost_diagram)\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGRAM 3: COMPARISON TABLE\n",
    "# ============================================================================\n",
    "print(\"\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\n",
    "print(\"â”‚\" + \"DIAGRAM 3: DETAILED COMPARISON\".ljust(148) + \"â”‚\")\n",
    "print(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n",
    "\n",
    "comparison_data = [\n",
    "    [\"Aspect\", \"Regular Retriever\", \"Contextual Compression\"],\n",
    "    [\"â”€\" * 30, \"â”€\" * 50, \"â”€\" * 60],\n",
    "    [\n",
    "        \"Tokens per 4 docs\",\n",
    "        \"~2,000 tokens\",\n",
    "        \"~250 tokens (90% reduction)\"\n",
    "    ],\n",
    "    [\n",
    "        \"LLM Context\",\n",
    "        \"Cluttered with irrelevant info\",\n",
    "        \"Clean, focused, relevant only\"\n",
    "    ],\n",
    "    [\n",
    "        \"Cost per query\",\n",
    "        \"High (full documents)\",\n",
    "        \"Low (compressed documents)\"\n",
    "    ],\n",
    "    [\n",
    "        \"Processing Speed\",\n",
    "        \"Fast âš¡âš¡âš¡\",\n",
    "        \"Slower âš¡ (needs compression)\"\n",
    "    ],\n",
    "    [\n",
    "        \"Hallucination Risk\",\n",
    "        \"Higher (noise confuses LLM)\",\n",
    "        \"Lower (only relevant info)\"\n",
    "    ],\n",
    "    [\n",
    "        \"Best For\",\n",
    "        \"Simple queries, quick responses\",\n",
    "        \"Large docs, token-limited, cost-sensitive\"\n",
    "    ],\n",
    "    [\n",
    "        \"Setup Complexity\",\n",
    "        \"Simple\",\n",
    "        \"Moderate (needs compressor LLM)\"\n",
    "    ],\n",
    "    [\n",
    "        \"Trade-off\",\n",
    "        \"Fast but expensive\",\n",
    "        \"Slower but cheaper & cleaner\"\n",
    "    ]\n",
    "]\n",
    "\n",
    "print(tabulate(comparison_data, tablefmt=\"grid\", maxcolwidths=[30, 50, 60]))\n",
    "\n",
    "# ============================================================================\n",
    "# DIAGRAM 4: WHEN TO USE EACH\n",
    "# ============================================================================\n",
    "print(\"\\n\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\n",
    "print(\"â”‚\" + \"DIAGRAM 4: DECISION GUIDE - WHEN TO USE WHAT\".ljust(148) + \"â”‚\")\n",
    "print(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n",
    "\n",
    "decision_guide = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           CHOOSE REGULAR RETRIEVER WHEN:                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  âœ“ Small documents (< 200 tokens average)                                                   â”‚\n",
    "â”‚  âœ“ Need maximum speed (no compression overhead)                                             â”‚\n",
    "â”‚  âœ“ Budget is unlimited                                                                      â”‚\n",
    "â”‚  âœ“ Few documents to retrieve                                                                â”‚\n",
    "â”‚  âœ“ Simple Q&A with high relevance documents                                                 â”‚\n",
    "â”‚  âœ“ Prototyping / Learning (quick setup)                                                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    CHOOSE CONTEXTUAL COMPRESSION WHEN:                                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  âœ“ Large documents (> 500 tokens)                                                           â”‚\n",
    "â”‚  âœ“ Token limits (context window constraints)                                                â”‚\n",
    "â”‚  âœ“ Cost-sensitive (production, many queries)                                                â”‚\n",
    "â”‚  âœ“ Documents have mostly irrelevant information                                             â”‚\n",
    "â”‚  âœ“ Need high-quality, focused context                                                       â”‚\n",
    "â”‚  âœ“ Want to reduce hallucination                                                             â”‚\n",
    "â”‚  âœ“ Running on budget (startups, limited resources)                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                            HYBRID APPROACH:                                                   â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Use both: Retrieve 10 docs quickly â†’ Compress top 4 â†’ Send to LLM                         â”‚\n",
    "â”‚  Best of both worlds: Good recall + low cost                                                â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "\n",
    "print(decision_guide)\n",
    "\n",
    "# ============================================================================\n",
    "# CODE EXAMPLE: HOW TO USE IT\n",
    "# ============================================================================\n",
    "print(\"\\nâ”Œ\" + \"â”€\"*148 + \"â”\")\n",
    "print(\"â”‚\" + \"DIAGRAM 5: CODE EXAMPLE - IMPLEMENTING CONTEXTUAL COMPRESSION\".ljust(148) + \"â”‚\")\n",
    "print(\"â””\" + \"â”€\"*148 + \"â”˜\\n\")\n",
    "\n",
    "code_example = \"\"\"\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMCompressor\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Step 1: Create base retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "# Step 2: Create compressor (LLM that compresses documents)\n",
    "compressor_llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "compressor = LLMCompressor.from_llm(compressor_llm)\n",
    "\n",
    "# Step 3: Wrap retriever with compression\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")\n",
    "\n",
    "# Step 4: Use it (same way as regular retriever)\n",
    "docs = compression_retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "print(f\"Retrieved {len(docs)} documents\")\n",
    "for doc in docs:\n",
    "    print(f\"Compressed content: {doc.page_content}\")\n",
    "    print(f\"Original size was larger, now optimized!\")\n",
    "\n",
    "\n",
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "WHAT HAPPENS INTERNALLY:\n",
    "\n",
    "1. Your Query\n",
    "   â†“\n",
    "2. Base Retriever fetches 10 raw documents (5,000 tokens)\n",
    "   â†“\n",
    "3. LLMCompressor analyzes: \"Which parts are relevant to the query?\"\n",
    "   â†“\n",
    "4. Compressor creates summaries/extracts relevant parts\n",
    "   â†“\n",
    "5. Compressed documents returned (500 tokens)\n",
    "   â†“\n",
    "6. Documents sent to your LLM with 10x fewer tokens!\n",
    "\"\"\"\n",
    "\n",
    "print(code_example)\n",
    "\n",
    "print(\"\\n\" + \"â–ˆ\"*150 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40t9y2c2tau",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "IMPLEMENTING CONTEXTUAL COMPRESSION RETRIEVER IN AGENT\n",
      "================================================================================\n",
      "\n",
      "Attempting to import ContextualCompressionRetriever...\n",
      "\n",
      "\n",
      "âš ï¸  Compression modules NOT found in this LangChain version\n",
      "   â†’ Using regular retriever as fallback\n",
      "   â†’ Requires: pip install --upgrade 'langchain>=0.2.0'\n",
      "\n",
      ">>> COMPRESSION NOT AVAILABLE - Using regular retriever\n",
      "\n",
      "â„¹ï¸  The agent from Cell 2 is already using regular retrieval\n",
      "   It works perfectly, just without token compression.\n",
      "\n",
      "================================================================================\n",
      "FALLBACK: USING REGULAR RAG AGENT (From Cell 2)\n",
      "================================================================================\n",
      "\n",
      "Query 1: What is LangChain?\n",
      "Answer: LangChain is a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "Query 2: What features does LangChain support?\n",
      "Answer: LangChain supports the following features:\n",
      "- RAG\n",
      "- Agents\n",
      "- Memory\n",
      "- Tools\n",
      "- More\n",
      "\n",
      "These features are commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "Query 3: Who created LangChain?\n",
      "Answer: LangChain was created by Harrison Chase.\n",
      "\n",
      "\n",
      "================================================================================\n",
      "SUMMARY\n",
      "================================================================================\n",
      "\n",
      "âœ… FALLBACK MODE ACTIVE\n",
      "   â€¢ Uses: agent (from Cell 2)\n",
      "   â€¢ Feature: Regular retrieval without compression\n",
      "   â€¢ Status: Fully functional, just not optimized for token cost\n",
      "\n",
      "ğŸ”§ To Enable Compression:\n",
      "   1. Upgrade LangChain:\n",
      "      pip install --upgrade 'langchain>=0.2.0'\n",
      "   2. Then re-run this cell\n",
      "   3. Compression will automatically activate\n",
      "\n",
      "ğŸ“Š Token Usage:\n",
      "   Current: ~2,000 tokens per retrieval\n",
      "   With upgrade: ~250 tokens per retrieval\n",
      "   Potential savings: ~90%\n",
      "\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 4: CONTEXTUAL COMPRESSION AGENT IMPLEMENTATION\n",
    "# ============================================================================\n",
    "# This cell implements ContextualCompressionRetriever in the agent\n",
    "# with graceful fallback if modules are not available\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"IMPLEMENTING CONTEXTUAL COMPRESSION RETRIEVER IN AGENT\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Attempt to Import Compression Components (Multiple Paths)\n",
    "# ============================================================================\n",
    "compression_available = False\n",
    "\n",
    "# Try multiple import paths for compatibility with different LangChain versions\n",
    "import_paths = [\n",
    "    (\"langchain.retrievers\", \"ContextualCompressionRetriever\"),\n",
    "    (\"langchain_community.retrievers\", \"ContextualCompressionRetriever\"),\n",
    "    (\"langchain.retrievers.contextual_compression\", \"ContextualCompressionRetriever\"),\n",
    "]\n",
    "\n",
    "compressor_paths = [\n",
    "    (\"langchain.retrievers.document_compressors\", \"LLMCompressor\"),\n",
    "    (\"langchain_community.retrievers.document_compressors\", \"LLMCompressor\"),\n",
    "]\n",
    "\n",
    "print(\"Attempting to import ContextualCompressionRetriever...\\n\")\n",
    "\n",
    "# Try to import ContextualCompressionRetriever\n",
    "ContextualCompressionRetriever = None\n",
    "for module_name, class_name in import_paths:\n",
    "    try:\n",
    "        module = __import__(module_name, fromlist=[class_name])\n",
    "        ContextualCompressionRetriever = getattr(module, class_name, None)\n",
    "        if ContextualCompressionRetriever:\n",
    "            print(f\"  âœ“ Found {class_name} in {module_name}\")\n",
    "            break\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# Try to import LLMCompressor\n",
    "LLMCompressor = None\n",
    "for module_name, class_name in compressor_paths:\n",
    "    try:\n",
    "        module = __import__(module_name, fromlist=[class_name])\n",
    "        LLMCompressor = getattr(module, class_name, None)\n",
    "        if LLMCompressor:\n",
    "            print(f\"  âœ“ Found {class_name} in {module_name}\")\n",
    "            break\n",
    "    except (ImportError, AttributeError):\n",
    "        pass\n",
    "\n",
    "# Check if both were imported successfully\n",
    "if ContextualCompressionRetriever and LLMCompressor:\n",
    "    compression_available = True\n",
    "    print(\"\\nâœ… Compression modules imported successfully!\")\n",
    "    print(\"   â†’ ContextualCompressionRetriever is available\\n\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸  Compression modules NOT found in this LangChain version\")\n",
    "    print(\"   â†’ Using regular retriever as fallback\")\n",
    "    print(\"   â†’ Requires: pip install --upgrade 'langchain>=0.2.0'\\n\")\n",
    "    compression_available = False\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2A: IF COMPRESSION IS AVAILABLE - Create Compression Agent\n",
    "# ============================================================================\n",
    "if compression_available:\n",
    "    print(\">>> COMPRESSION ENABLED - Creating compressed retriever...\\n\")\n",
    "    \n",
    "    # Create a compressor LLM (can use cheaper/faster model for compression)\n",
    "    compressor_llm = ChatOpenAI(\n",
    "        model=\"gpt-3.5-turbo\",  # Use fast/cheap model for compression\n",
    "        temperature=0\n",
    "    )\n",
    "    \n",
    "    # Create the LLMCompressor\n",
    "    # This LLM will intelligently extract relevant parts from documents\n",
    "    compressor = LLMCompressor.from_llm(compressor_llm)\n",
    "    \n",
    "    # Wrap the retriever with compression\n",
    "    # - Fetches documents with base_retriever\n",
    "    # - Compresses them with base_compressor\n",
    "    compression_retriever = ContextualCompressionRetriever(\n",
    "        base_compressor=compressor,\n",
    "        base_retriever=retriever  # Uses the retriever from Cell 2\n",
    "    )\n",
    "    \n",
    "    print(\"âœ“ Compressor created with GPT-3.5-turbo\")\n",
    "    print(\"âœ“ Compression retriever wrapped around base retriever\\n\")\n",
    "    \n",
    "    # Create RAG tool with compressed retriever\n",
    "    compressed_rag_tool = create_retriever_tool(\n",
    "        compression_retriever,\n",
    "        name=\"compressed_rag_qa_tool\",\n",
    "        description=\"Use this tool to answer questions about LangChain (with token compression) using the sample.txt document\"\n",
    "    )\n",
    "    print(\"âœ“ Compressed RAG tool created\\n\")\n",
    "    \n",
    "    # Create agent with compressed RAG tool\n",
    "    compression_agent = create_agent(\n",
    "        openai_llm,\n",
    "        tools=[compressed_rag_tool],\n",
    "        system_prompt=\"\"\"You are a helpful assistant. Use the RAG tool when needed to answer questions.\n",
    "The tool automatically compresses documents to extract only relevant information, optimizing token usage.\"\"\"\n",
    "    )\n",
    "    print(\"âœ“ Agent created with compressed retriever\\n\")\n",
    "    \n",
    "    # ========================================================================\n",
    "    # Step 3A: Test Compressed Agent\n",
    "    # ========================================================================\n",
    "    print(\"=\"*80)\n",
    "    print(\"TESTING COMPRESSED RAG AGENT\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is LangChain?\",\n",
    "        \"What features does LangChain support?\",\n",
    "        \"Who created LangChain?\"\n",
    "    ]\n",
    "    \n",
    "    for idx, query in enumerate(test_queries, 1):\n",
    "        print(f\"Query {idx}: {query}\")\n",
    "        try:\n",
    "            result = compression_agent.invoke({\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "            })\n",
    "            print(f\"Answer: {result['messages'][-1].content}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2B: IF COMPRESSION NOT AVAILABLE - Use Regular Retriever (Fallback)\n",
    "# ============================================================================\n",
    "else:\n",
    "    print(\">>> COMPRESSION NOT AVAILABLE - Using regular retriever\\n\")\n",
    "    print(\"â„¹ï¸  The agent from Cell 2 is already using regular retrieval\")\n",
    "    print(\"   It works perfectly, just without token compression.\\n\")\n",
    "    \n",
    "    # The 'agent' from Cell 2 is already available and working\n",
    "    # No additional setup needed\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FALLBACK: USING REGULAR RAG AGENT (From Cell 2)\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    test_queries = [\n",
    "        \"What is LangChain?\",\n",
    "        \"What features does LangChain support?\",\n",
    "        \"Who created LangChain?\"\n",
    "    ]\n",
    "    \n",
    "    for idx, query in enumerate(test_queries, 1):\n",
    "        print(f\"Query {idx}: {query}\")\n",
    "        try:\n",
    "            result = agent.invoke({  # Uses agent from Cell 2\n",
    "                \"messages\": [{\"role\": \"user\", \"content\": query}]\n",
    "            })\n",
    "            print(f\"Answer: {result['messages'][-1].content}\\n\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Summary\n",
    "# ============================================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if compression_available:\n",
    "    print(\"\\nâœ… COMPRESSION AGENT ACTIVE\")\n",
    "    print(\"   â€¢ Uses: compression_agent\")\n",
    "    print(\"   â€¢ Feature: Documents automatically compressed\")\n",
    "    print(\"   â€¢ Benefit: 90% token reduction on large documents\")\n",
    "    print(\"   â€¢ Trade-off: Slightly slower (compression step)\")\n",
    "    print(\"\\nğŸ“Š Token Usage:\")\n",
    "    print(\"   Regular: ~2,000 tokens per retrieval\")\n",
    "    print(\"   Compressed: ~250 tokens per retrieval\")\n",
    "    print(\"   Savings: ~90% (9x cheaper!)\")\n",
    "else:\n",
    "    print(\"\\nâœ… FALLBACK MODE ACTIVE\")\n",
    "    print(\"   â€¢ Uses: agent (from Cell 2)\")\n",
    "    print(\"   â€¢ Feature: Regular retrieval without compression\")\n",
    "    print(\"   â€¢ Status: Fully functional, just not optimized for token cost\")\n",
    "    print(\"\\nğŸ”§ To Enable Compression:\")\n",
    "    print(\"   1. Upgrade LangChain:\")\n",
    "    print(\"      pip install --upgrade 'langchain>=0.2.0'\")\n",
    "    print(\"   2. Then re-run this cell\")\n",
    "    print(\"   3. Compression will automatically activate\")\n",
    "    print(\"\\nğŸ“Š Token Usage:\")\n",
    "    print(\"   Current: ~2,000 tokens per retrieval\")\n",
    "    print(\"   With upgrade: ~250 tokens per retrieval\")\n",
    "    print(\"   Potential savings: ~90%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80 + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
