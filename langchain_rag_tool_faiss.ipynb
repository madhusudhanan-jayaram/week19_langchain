{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f250c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key loaded: AIzaSy...\n",
      "OpenAI Key loaded: sk-pro...\n",
      "OpenAI response: content='\"IntelliHome Innovations\"' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 21, 'total_tokens': 28, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-CrZD9o5uLDwCLQMKBoCKYrVxewGC9', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='lc_run--019b626b-811f-7fe1-b6eb-b29f74da2d1c-0' usage_metadata={'input_tokens': 21, 'output_tokens': 7, 'total_tokens': 28, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 1: Imports and LLM Configuration\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Core Imports\n",
    "# ============================================================================\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# ============================================================================\n",
    "# Text Processing & Embeddings\n",
    "# ============================================================================\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# ============================================================================\n",
    "# Vector Stores & Retrieval\n",
    "# ============================================================================\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.tools import create_retriever_tool\n",
    "\n",
    "# ============================================================================\n",
    "# Language Models (LLMs)\n",
    "# ============================================================================\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# ============================================================================\n",
    "# Prompts & Agents\n",
    "# ============================================================================\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "# ============================================================================\n",
    "# Memory & Conversation (for RAG applications)\n",
    "# ============================================================================\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# ============================================================================\n",
    "# API Key Configuration\n",
    "# ============================================================================\n",
    "# Retrieve API keys for Gemini and OpenAI from environment variables\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Validate that both API keys are present\n",
    "assert GEMINI_API_KEY, \"GEMINI_API_KEY is missing. Check your .env file.\"\n",
    "assert OPENAI_API_KEY, \"OPENAI_API_KEY is missing. Check your .env file.\"\n",
    "\n",
    "# Display partial keys to confirm successful loading (for security, only show first 6 chars)\n",
    "print(\"Key loaded:\", GEMINI_API_KEY[:6] + \"...\" )\n",
    "print(\"OpenAI Key loaded:\", OPENAI_API_KEY[:6] + \"...\")\n",
    "\n",
    "# ============================================================================\n",
    "# Initialize Language Models (LLMs)\n",
    "# ============================================================================\n",
    "# Initialize Google's Gemini LLM with temperature=0 for deterministic responses\n",
    "gemini_llm = ChatGoogleGenerativeAI(\n",
    "    temperature=0,  # Deterministic output (no randomness)\n",
    "    model=\"gemini-2.5-flash\",  # Using the faster Gemini model\n",
    "    google_api_key=GEMINI_API_KEY\n",
    ")\n",
    "\n",
    "# Initialize OpenAI's GPT-3.5-turbo LLM\n",
    "openai_llm = ChatOpenAI(\n",
    "    temperature=0,  # Deterministic output\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Create a Simple Prompt Template\n",
    "# ============================================================================\n",
    "# Define a reusable prompt template for company name generation\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],  # Variable to be filled dynamically\n",
    "    template=\"Give me a creative name for a company that makes {product}?\",\n",
    ")\n",
    "\n",
    "# ============================================================================\n",
    "# Example 1: LLM Chain (Prompt → LLM)\n",
    "# ============================================================================\n",
    "# Create a chain: prompt template piped to OpenAI LLM\n",
    "chain = prompt | openai_llm\n",
    "\n",
    "# Execute the chain with a specific product type\n",
    "response = chain.invoke({\"product\": \"smart home devices\"})\n",
    "\n",
    "# Display the generated company name\n",
    "print(\"OpenAI response:\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d11f1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading document from sample.txt...\n",
      "✓ Document loaded successfully\n",
      "\n",
      "Splitting text into chunks...\n",
      "✓ Created 1 document chunks\n",
      "\n",
      "Creating embeddings and FAISS vector store...\n",
      "✓ FAISS vector store created successfully\n",
      "\n",
      "Creating retriever...\n",
      "✓ Retriever configured (returns top 4 documents)\n",
      "\n",
      "Creating RAG tool...\n",
      "✓ RAG tool created successfully\n",
      "\n",
      "Initializing agent with RAG tool...\n",
      "✓ Agent created successfully\n",
      "\n",
      "================================================================================\n",
      "TESTING RAG AGENT WITH FAISS VECTOR STORE\n",
      "================================================================================\n",
      "\n",
      "Query: What is LangChain?\n",
      "Answer: LangChain is a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow.\n",
      "\n",
      "Query: Why do we use LangChain?\n",
      "Answer: LangChain is used as a framework for building applications with Large Language Models (LLMs). It was created by Harrison Chase and supports various features such as RAG, agents, memory, tools, and more. LangChain is commonly used in chatbots, document Q&A, and AI workflow applications.\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# LangChain RAG Tool Setup - Cell 2: FAISS Vector Database & Retriever\n",
    "# ============================================================================\n",
    "\n",
    "# ============================================================================\n",
    "# Step 1: Load Document\n",
    "# ============================================================================\n",
    "# Read the sample.txt file containing text data to be indexed\n",
    "print(\"Loading document from sample.txt...\")\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "print(\"✓ Document loaded successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 2: Split Text into Chunks\n",
    "# ============================================================================\n",
    "# Initialize text splitter to break documents into manageable chunks\n",
    "# - separator=\"\\n\": Split on newline characters\n",
    "# - chunk_size=300: Each chunk contains ~300 characters\n",
    "# - chunk_overlap=50: Overlap for context preservation\n",
    "print(\"Splitting text into chunks...\")\n",
    "splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "# Create document objects from text\n",
    "docs = splitter.create_documents([text_data])\n",
    "print(f\"✓ Created {len(docs)} document chunks\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 3: Create Embeddings and FAISS Vector Store\n",
    "# ============================================================================\n",
    "# Initialize OpenAI embeddings (converts text to vectors)\n",
    "print(\"Creating embeddings and FAISS vector store...\")\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Create FAISS vector database from documents\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "print(\"✓ FAISS vector store created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 4: Create Retriever\n",
    "# ============================================================================\n",
    "# Create a retriever that fetches relevant documents\n",
    "# search_kwargs={\"k\": 4} means return top 4 most relevant documents\n",
    "print(\"Creating retriever...\")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"✓ Retriever configured (returns top 4 documents)\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 5: Convert Retriever to RAG Tool\n",
    "# ============================================================================\n",
    "# Wrap the retriever as a tool for the agent to use\n",
    "print(\"Creating RAG tool...\")\n",
    "rag_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    name=\"rag_qa_tool\",\n",
    "    description=\"Use this tool to answer questions about LangChain using the sample.txt document\"\n",
    ")\n",
    "print(\"✓ RAG tool created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 6: Create Agent with RAG Tool\n",
    "# ============================================================================\n",
    "# Create modern agent that can use the RAG tool\n",
    "print(\"Initializing agent with RAG tool...\")\n",
    "agent = create_agent(\n",
    "    openai_llm,  # Use OpenAI LLM\n",
    "    tools=[rag_tool],  # Provide RAG tool\n",
    "    system_prompt=\"You are a helpful assistant. Use the RAG tool when needed to answer questions accurately based on the provided documents.\"\n",
    ")\n",
    "print(\"✓ Agent created successfully\\n\")\n",
    "\n",
    "# ============================================================================\n",
    "# Step 7: Test the RAG Agent\n",
    "# ============================================================================\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING RAG AGENT WITH FAISS VECTOR STORE\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "# Test Query 1\n",
    "query1 = \"What is LangChain?\"\n",
    "print(f\"Query: {query1}\")\n",
    "result1 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query1}]\n",
    "})\n",
    "print(f\"Answer: {result1['messages'][-1].content}\\n\")\n",
    "\n",
    "# Test Query 2\n",
    "query2 = \"Why do we use LangChain?\"\n",
    "print(f\"Query: {query2}\")\n",
    "result2 = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": query2}]\n",
    "})\n",
    "print(f\"Answer: {result2['messages'][-1].content}\\n\")\n",
    "\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
